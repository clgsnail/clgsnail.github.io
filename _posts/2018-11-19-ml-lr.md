---
layout: post
title: "LR数学原理与python实现"
subtitle: 'The Mathematical Principles and Python Implementation of LR'
author: "Quanli"
header-style: text
catalog: true
mathjax: true
tags:
  - machine learning
  - logistic regression
---

## LR数学原理
LR数学原理主要涉及LR数学模型的搭建以及模型参数的确定。

### LR数学模型的搭建
从概率的角度来看输出结果，即考虑输出结果的不确定性，我们使用$sigmoid$函数来作为LR输出的激活函数，$sigmoid$如下所示：

$$
 h_ \theta (x) = \frac{\mathrm{1} }{\mathrm{1} + e^{- \theta^Tx }}
$$

其中，$sigmoid$的取值范围：$$ 0 \le h_ \theta (x) \le 1 $$

#### Logistic Regression(LR)
二项LR是如下的条件概率分布：

$$P(Y=1|x)=\frac{exp(\omega \cdot x + b)}{1 + exp(\omega \cdot x + b)}$$

$$P(Y=0|x)=\frac{\mathrm{1}}{1 + exp(\omega \cdot x + b)}$$

$x\in \mathbf{R}^n$是输入，$Y\in\{0,1\}$是输出，$\omega \in \mathbf{R}^n$和$b \in \mathbf{R}$是参数，$\omega$是权值向量，$b$是偏置，$\omega \cdot x$为$\omega$和$x$的内积。

LR通过对比两个条件概率值的大小，将实例$x$分到概率值较大的那一类。

#### 模型参数估计
通过最大似然估计发来估计模型参数，从而得到LR模型。
设：

$$P(Y=1|x)=\pi(x)$$

$$P(Y=0|x)=1-\pi(x)$$

似然函数为：

$$\prod_{i-1}^{N}[\pi(x_i)]^y_i[1-\pi(x_i)]^{1-y_i} $$

对数似然函数为：

$$
\begin{align}
L(\omega)&=\sum_{i-1}^{N}[y_ilog\pi(x_i)+(1-y_i)log(1-\pi(x_i))]  \\
&=\sum_{i=1}^{N}\left[y_ilog\frac{\pi(x_i)}{1-\pi(x_i)}+log(1-\pi(x_i))\right] \\
&=\sum_{i=1}^{N}[y_i(\omega \cdot x_i)-log(1+exp(\omega \cdot x_i))]
\end{align}
$$

对$L(\omega)$求极大值，得到$\omega$的估计值。

$$
\begin{align}
\frac{\partial L(\omega)}{\partial \omega} &= \sum_{i=1}^{N} y_ix_i - \sum_i^N \frac{e^{\omega ^ T x_i}}{1 + e^{\omega ^ T x_i}}  \\
&=\sum_{i=1}^{N}(y_i - \sigma (\omega x_i))x_i
\end{align}
$$

此式没有解析解，因此需要使用迭代法，借助梯度下降法。

因此，问题变成了以对数似然函数为目标函数的最优化问题，LR学习中通常采用的方法是梯度下降法及拟牛顿法。

## [LR python实现](https://blog.csdn.net/zouxy09/article/details/20319673)
首先实现$sigmoid$函数：

```js
def sigmoid(x):
    return 1. / (1+exp(-x))
```

接着，梯度下降如下所示：

```js
def gradientDescent(train_x, train_y):
    weights = numpy.ones((numFeatures, 1))
    
    output = sigmoid(train_x * weights)
    error = train_y - output
    weights = weights + alpha * train_x.transpose() * error
```

随机梯度下降如下所示：

```js
def stocGradientDescent(train_x, train_y):
    weights = numpy.ones((numFeatures, 1))

    for i in range(numSamples):
        output = sigmoid(train_x[i, :] * weights)
        error = train_y[i, 0] - output
        weights = weights + alpha * train_x[i, :].transpose() * error
```

---
