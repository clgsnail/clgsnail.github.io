---
layout: post
title: "DT数学原理与python实现"
subtitle: 'The Mathematical Principles and Python Implementation of DT'
author: "Quanli"
header-style: text
catalog: true
mathjax: true
tags:
  - machine learning
  - decision tree
---

Decision Tree(DT)主要包括：
* 特征选择
* 决策树的生成
* 决策树的修建

## 特征选择
特征选择主要通过特征给模型带来的信息增益，特征信息增益越大表名该特征越重要。

### 信息熵
在信息论与概率统计中，熵是表示随机变量不确定性的度量。

设$X$是一个有限个值的离散随机变，其概率分布为：

$$
P(X=x_i)=p_i, \quad i=1,2, \cdots ,n
$$

则随机变量$X$的熵定义为：

$$
H(X)=- \sum_{i=1}^N p_ilogp_i
$$

熵越大，随机变量的不确定性越大，其中：

$$
0\leq H(p) \leq logn
$$

#### 条件熵
设随机变量$(X,Y)$，其联合概率分布为：

$$
P(X=x_i, Y=y_j)=p_{ij}, \quad i=1,2, \cdots ,n; \quad j=1,2, \cdots ,m
$$

条件熵$H(Y\|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。

随机变量$X$给定的条件下随机变量$Y$的条件熵$H(Y\|X)$，定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望：

$$
H(Y|X)=\sum_{i-1}^{N}p_iH(Y|X=x_i)
$$

这里，$p_i=P(X=x_i), \quad i=1, 2, \cdots , N$

#### 经验熵、经验条件熵
当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵和条件熵分别称为经验熵和经验条件熵。

### 信息增益
定义：特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D\|A)$之差，即：

$$
g(D,A)=H(D)-H(D|A)
$$

决策树学习应用信息增益准则选择特征，给定训练数据集$D$和特征$A$，经验熵$H(D)$表示数据集$D$进行分类的不确定性。而经验条件熵$H(D\|A)$表示在特征$A$给定的条件下对数据集$D$进行分类的不确定性，那么它们的差，即信息增益，就表示由于特征$A$而使得对数据集$D$的分类的不确定性，那么它们的差，即信息增益，表示由于特征$A$而使得对数据集$D$的分类的不确定性减少的程度。

显然，对于数据集$D$而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益。信息增益大的特征具有更强的分类能力。


根据信息增益准则的特征选择方法是：对训练数据集（或子集）$D$，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。

### 信息增益比
以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题，使用信息增益比可以修正该问题。

定义：特征$A$对训练数据集$D$的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比，即：

$$
g_R(D, A)=\frac{g(D,A)}{H_A(D)}
$$

其中，$H_A(D)=-\sum_{i=1}^N \frac{\|D_i\|}{\|D\|}log_2 \frac{\|D_i\|}{\|D\|}$，$n$是特征$A$取值的个数。

## 决策树的生成

### C4.5的生成算法

**C4.5的生成算法**  
输入：训练数据集$D$，特征集$A$，阈值$\varepsilon$;  
输出：决策树$T$。  
（1）如果$D$中所有实例属于同一类$C_k$，则置$T$为单结点树，并将$C_k$作为该结点的类，返回$T$；  
（2）如果$A=\varnothing$，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该结点的类，返回$T$；  
（3）否则，计算$A$中各特征对$D$的信息增益比，选择信息增益比最大的特征$A_g$；  
（4）如果$A_g$的信息增益比小于阈值$\varepsilon$，则置$T$为单结点树，并将$D$中实例数最大的类$C_k$作为该节点的类，返回$T$；  
（5）否则，对$A_g$的每一可能值$a_i$，依$A_g=a_i$将$D$分割为子集若干非空$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$；  
（6）对结点$i$，以$D_i$为训练集，以$A-{A_g}$为特征集，递归地调用步（1）$\sim$步（5）  

## 决策树的剪枝

决策树的剪枝往往通过极小化决策树整体的损失函数来实现，设树$T$的叶节点个数为$\|T\|$，$t$是树$T$的叶结点，该叶结点有$N_t$个样本结点，其中$k$类的样本点有$N_{tk}$个，$k=1,2, \cdots ,K$，$H_t(T)$为叶结点$t$上到的经验熵，$\alpha \ge 0$为参数，则决策树学习的损失函数可定义为：

$$
C_{\alpha}(T)=\sum_{t-1}^{|T|}N_tH_t(T)+ \alpha |T|
$$

其中经验熵为：

$$
H_t(T)=-\sum_{k} \frac{N_{tk}}{N_t}log \frac{N_{tk}}{N_t}
$$

其中：

$$
C(T)=\sum_{t-1}^{|T|}N_tH_t(T)=-\sum_{t-1}^{|T|}\sum_{k=1}^{K}N_{tk} log \frac{N_{tk}}{N_t}
$$

这时有：

$$
C_{\alpha}(T)=C(T)+ \alpha |T|
$$


## Python实现

在此使用Python实现CART算法

```js

```

---
